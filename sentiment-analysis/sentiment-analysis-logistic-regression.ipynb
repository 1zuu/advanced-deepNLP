{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3372b759-e6a9-4ed1-bea6-a2a18ad40660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7126f90-f656-4fd9-84b7-d9a51ab48bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "file_name = 'data/tweets.csv'\n",
    "headers = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "device =  torch.device('cpu' if not torch.cuda.is_available() else 'cuda:0') \n",
    "batch_size = 128\n",
    "random_state = 123\n",
    "\n",
    "print(\"Device : {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e7e4fe-5c3f-4a25-a431-edb8bcf5de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_namet, down_sample=True):\n",
    "    data = pd.read_csv(\n",
    "                    file_name,\n",
    "                    names=headers,\n",
    "                    encoding='latin-1'\n",
    "                    )\n",
    "    \n",
    "    data = shuffle(data)\n",
    "    \n",
    "    data = data[['target', 'text']]\n",
    "    data['target'] = data['target'].astype(int)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    if down_sample:\n",
    "        data = data.sample(\n",
    "                        frac=0.01, \n",
    "                        replace=False, \n",
    "                        random_state=random_state\n",
    "                        ) # Since I have Limited Resources I downsample the dataset heavily\n",
    "    return data\n",
    "\n",
    "def lemmatization(lemmatizer,sentence):\n",
    "    lem = [lemmatizer.lemmatize(k) for k in sentence]\n",
    "    return [k for k in lem if k]\n",
    "\n",
    "def remove_stop_words(stopwords_list,sentence):\n",
    "    return [k for k in sentence if k not in stopwords_list]\n",
    "\n",
    "def preprocess_one(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    remove_punc = tokenizer.tokenize(tweet) # Remove puntuations\n",
    "    remove_num = [re.sub('[0-9]', '', i) for i in remove_punc] # Remove Numbers\n",
    "    remove_num = [i for i in remove_num if len(i)>0] # Remove empty strings\n",
    "    lemmatized = lemmatization(lemmatizer,remove_num) # Word Lemmatization\n",
    "    remove_stop = remove_stop_words(stopwords_list,lemmatized) # remove stop words\n",
    "    updated_tweet = ' '.join(remove_stop)\n",
    "    return updated_tweet\n",
    "\n",
    "def preprocessed_data(tweets):\n",
    "    updated_tweets = []\n",
    "    if isinstance(tweets, np.ndarray) or isinstance(tweets, list):\n",
    "        for tweet in tweets:\n",
    "            updated_tweet = preprocess_one(tweet)\n",
    "            updated_tweets.append(updated_tweet)\n",
    "    elif isinstance(tweets, np.str_)  or isinstance(tweets, str):\n",
    "        updated_tweets = [preprocess_one(tweets)]\n",
    "\n",
    "    return np.array(updated_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f765450-6b1e-4969-bf49-c0b474a5d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(file_name)\n",
    "\n",
    "X = data.text.values\n",
    "Y = data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e41b0342-deff-4a6a-83f9-35e3ac517ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "\n",
    "Y = encoder.transform(Y)\n",
    "X = preprocessed_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb0a7b6d-d910-4db4-af22-beae15b18fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aunt watching wee cousin mum aunt looking one done bunk',\n",
       "       'evan_wells well deserved live demo wa one highlight e point view keep great work',\n",
       "       'rip david eddings', ..., 'hwy another part country',\n",
       "       'bekakeb thank yaa bekkaa',\n",
       "       'listening chemical romance dad bracelet got wet iranelection'],\n",
       "      dtype='<U217')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "532ba4da-5956-4123-91c6-f0b3a4acbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary():\n",
    "    vocabulary = {}\n",
    "    for x in X:\n",
    "        tokens = x.split(' ')\n",
    "        for token in tokens:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = 1\n",
    "            else:\n",
    "                vocabulary[token] += 1\n",
    "                \n",
    "    vocabulary = {k: v for k, v in sorted(vocabulary.items(), key=lambda item: item[1], reverse=True)}\n",
    "    vocabulary = {k : idx+1 for idx, k in enumerate(vocabulary.keys())}\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f047052-6671-4577-9b50-b8fbc72a6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = extract_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71cfe307-faf0-4973-b51e-873ccf80e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_tokens(label):\n",
    "    Xc = X[Y==label]\n",
    "    Xc_tokens = []\n",
    "\n",
    "    for xc in Xc:\n",
    "        xc_tokens = xc.split(' ') \n",
    "        Xc_tokens.extend(xc_tokens)\n",
    "        \n",
    "    return Xc_tokens\n",
    "        \n",
    "def extract_frequencies():\n",
    "    frequency_dict = {}\n",
    "\n",
    "    Xpos_tokens = extract_all_tokens(1)\n",
    "    Xneg_tokens = extract_all_tokens(0)\n",
    "    \n",
    "    print('Class-wise Tokens Extracted')\n",
    "    \n",
    "    for token in vocabulary.keys():\n",
    "        frequency_dict[(token, 1)] = Xpos_tokens.count(token)\n",
    "        frequency_dict[(token, 0)] = Xneg_tokens.count(token)\n",
    "        \n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1145560-e5ce-42bb-9eff-e880cb7151fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise Tokens Extracted\n"
     ]
    }
   ],
   "source": [
    "frequency_dict = extract_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59460094-4d79-4917-816d-bd8d546c6459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features():\n",
    "    X_features = torch.empty(len(X), 3, dtype=torch.float32)\n",
    "    for idx, x in enumerate(X):\n",
    "        x_tokens = x.split(' ') \n",
    "        pos_freq_sum = sum([frequency_dict[(token, 1)] for token in x_tokens])\n",
    "        neg_freq_sum = sum([frequency_dict[(token, 0)] for token in x_tokens])\n",
    "        x_features = torch.tensor([1.0, pos_freq_sum, neg_freq_sum])\n",
    "        \n",
    "        X_features[idx, :] = x_features\n",
    "        \n",
    "    return X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06a3366f-bbd2-4a20-bc13-b363d191657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm = extract_features()\n",
    "Ym = torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c6047be-ce14-43b7-8d6e-bd3f032d9711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16000, 3]), torch.Size([16000]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xm.shape, Ym.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a04214c3-616b-4b28-96ea-a8d038121b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    Logistic Regression Equation:\n",
    "            \n",
    "        P = sig(Xm * Θ)\n",
    "        \n",
    "        Xm.shape = 16000, 3\n",
    "        Ym.shape = 16000\n",
    "        \n",
    "        Θ.shape = (3)\n",
    "\n",
    "'''\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + torch.exp(-Z))\n",
    "\n",
    "def logistic_regression(Xbatch, Θ):\n",
    "    return sigmoid(torch.matmul(Xbatch, Θ))\n",
    "\n",
    "def gradient(Xbatch, Ybatch, Pbatch):\n",
    "    return torch.mean(torch.matmul(Xbatch.T, Pbatch - Ybatch))\n",
    "    \n",
    "def weight_update(Θ, dΘ, alpha=0.01):\n",
    "    Θ = Θ - alpha * dΘ\n",
    "    return Θ\n",
    "\n",
    "def loss(Xbatch, Ybatch, Pbatch):\n",
    "    return torch.mean(torch.matmul(Xbatch.T, Pbatch - Ybatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3154c690-ecd9-4ef3-b85b-5cfc16471d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3539218.2500)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ = torch.rand(3)\n",
    "P = logistic_regression(Xm, Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468f377-a2b9-410f-a5cc-1045c2d92b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
